import keras
import os
import PIL
import glob
import cv2
from keras.models import load_model
import keras.backend as K
from os import listdir
import numpy as np
import PIL.Image as pilimg
import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, BatchNormalization, Dense, Flatten
import matplotlib.pylab as plt
import pandas as pd
from keras import backend as K
from keras import optimizers

Train_path = r'C:\Users\User\Desktop\보안-\text-classification-tutorial-master\android_parsing\total_data.csv'
x_train = list()
y_train = list()

import csv
f = open(Train_path, 'r')
csvReader = csv.reader(f)

for row in csvReader:
    x_train.append(row[1])
    y_train.append(row[0])
f.close()

for i in range(len(y_train)):#문자열은 test를 못하니까, normal 이면 0으로, malware면 1로
    if y_train[i] == 'normal':
        y_train[i] = 0
    elif y_train[i] == 'malware':
        y_train[i] = 1

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
x_train = vectorizer.fit_transform(x_train)
x_train = x_train.toarray()
x_train = np.array(x_train)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.15, random_state=0) #(15% 테스트로)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=0) #(15% val로)

nb_filters = 8
nb_kernels = 10
nb_strides = 1

x_train = x_train.reshape((-1, len(x_train[0]), 1))#입력 모양 맞춰주기
x_val = x_val.reshape((-1, len(x_val[0]), 1))
x_test = x_test.reshape((-1, len(x_test[0]), 1))

model = Sequential()
#model.add(Conv1D(filters=nb_filters, kernel_size=nb_kernels, strides=nb_strides, activation='relu',input_shape=(len(x_train[0]),1), padding = 'same'))
model.add(Conv1D(filters=nb_filters, kernel_size=nb_kernels, strides=nb_strides, activation='relu',input_shape=(len(x_train[0]),1)))
model.add(BatchNormalization())
#model.add(Conv1D(filters=nb_filters*2, kernel_size=nb_kernels, strides=nb_strides, activation='relu', padding = 'same'))
model.add(Conv1D(filters=nb_filters*2, kernel_size=nb_kernels, strides=nb_strides, activation='relu'))
model.add(BatchNormalization())
#model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu', padding = 'same'))
model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu'))
model.add(BatchNormalization())
#model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu', padding = 'same'))
#model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu'))
#model.add(BatchNormalization())
#model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu', padding = 'same'))
#model.add(BatchNormalization())
#model.add(Conv1D(filters=nb_filters*4, kernel_size=nb_kernels, strides=nb_strides, activation='relu', padding = 'same'))
#model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(units=1000, activation='relu'))
model.add(Dense(units=500, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
#model = keras.applications.VGG16(include_top=True, weights=None, input_tensor=None, input_shape=(224, 224, 3), pooling='max', classes=1)
model.summary()
#load_weight

SGD = optimizers.SGD(lr=0.001, decay = 0.005, momentum=0.9)
#Adam = optimizers.Adam(lr=0.001)
model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])

# this is the augmentation configuration we will use for training

from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=200, verbose=1, mode='auto')
tb_hist = keras.callbacks.TensorBoard(log_dir = r'C:\Users\User\.PyCharmCE2017.3\config\scratches\graph', histogram_freq = 0, write_graph = True, write_images = True)
checkpointer = ModelCheckpoint(filepath=r'C:\Users\User\.PyCharmCE2017.3\config\scratches\tmp\permission_1212.h5', monitor ='val_loss', verbose=0, save_best_only=True)
callback_list = [early_stopping, tb_hist, checkpointer]

batch_size = 16
model.fit(x=x_train, y=y_train, epochs=5000, batch_size=batch_size, validation_data=(x_val, y_val),callbacks = callback_list)

#load model
model = load_model(r'C:\Users\User\.PyCharmCE2017.3\config\scratches\tmp\permission_1212.h5')
predict = model.evaluate(x_test, y_test)
print(predict)